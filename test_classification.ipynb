{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_classification.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqnB5QAEZorn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41kUGZWwZxAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install impyute\n",
        "from impyute.imputation.cs import mice\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew, norm\n",
        "from scipy.special import boxcox1p\n",
        "from sklearn.feature_selection import mutual_info_classif, GenericUnivariateSelect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F5NSUGGZ3uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df_X_og = pd.read_csv('/content/drive/My Drive/Colab Notebooks/X_train.csv')\n",
        "df_Y_og = pd.read_csv('/content/drive/My Drive/Colab Notebooks/y_train.csv')\n",
        "df_X = df_X_og.copy()\n",
        "df_Y = df_Y_og.copy()\n",
        "print(df_X.shape)\n",
        "print(df_Y.shape)\n",
        "df_test_og = pd.read_csv('/content/drive/My Drive/Colab Notebooks/X_test.csv')\n",
        "df_test = df_test_og.copy()\n",
        "print(df_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJogZ1DdheSA",
        "colab_type": "text"
      },
      "source": [
        "## Greeting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNHlYkPsm3rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_X.head()\n",
        "#df_Y.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbIg37KIaphF",
        "colab_type": "text"
      },
      "source": [
        "Just by glancing over the difference in 75% and max values it is clearly visible that some columns are highly skewed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWprxjyGpMZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_X.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwNxvgMCZWRg",
        "colab_type": "text"
      },
      "source": [
        "Transforming the categorical int column types to object and bool to int"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SGWgeGy-Wg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfs = [df_X, df_test]\n",
        "print(df_X.info())\n",
        "print(df_test.info())\n",
        "for df in dfs:\n",
        "  categorical_col = df.columns[df.columns.str.startswith('C')]\n",
        "  for col in categorical_col:\n",
        "    if df[col].dtypes == bool:\n",
        "      df[col] = df[col].astype(int)\n",
        "    else:\n",
        "      df[col] = df[col].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MEfBCCVhyif",
        "colab_type": "text"
      },
      "source": [
        "## Imputing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xa0xlvPcwp3",
        "colab_type": "text"
      },
      "source": [
        "The stategy I used for imputing data is MICE(Multivariate Imputation by Chained Equations)using impyute module. I tried applying it to the whole data frame but it was super slow. There are few columns with more than 80% missing values, I am aware I could substitute the missing values with some string and binned the rest of the data finally hot encoding those columns but here I preferred to just drop them. After dropping them I tried MICE on rest of the dataframe but it was still slow so I decided to use median to impute the mising values in columns with missing percentage less than 13%. Then I applied MICE on the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7xiEVCDZ_YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for df in dfs:\n",
        "  all_missing_data = (df.isnull().sum()/len(df))*100\n",
        "  all_missing_data = all_missing_data.drop(all_missing_data[all_missing_data == 0].index).sort_values(ascending = False)\n",
        "  print(all_missing_data)\n",
        "  cols = all_missing_data[all_missing_data.values < 13].index\n",
        "  for col in cols:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "  cols_emp = all_missing_data[all_missing_data.values > 80].index\n",
        "  df.drop(cols_emp, axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rGvk1JIjMdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for df in dfs:\n",
        "  num_cols = df.select_dtypes(exclude=['object']).columns\n",
        "  filled = mice(df.loc[:,num_cols].values, seed = 103)\n",
        "  df_temp = pd.DataFrame(data = filled)\n",
        "  df_temp.columns = num_cols\n",
        "  df.update(df_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUgVl8zmdnV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.merge(df_X, df_Y, on = 'Unique_ID', how = 'inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCzqCBprlKE7",
        "colab_type": "text"
      },
      "source": [
        "I tried to boxcox the tackle the skewness but there are ton of negative values and boxcox1p return NaN for them. There is an alternate by using log(x - min value of column) but I skipped that for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGoGWIX3bhQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''dfs = [df_train, df_test]\n",
        "for df in dfs:\n",
        "  non_obj_col = df.columns[df.columns.str.startswith('N')]\n",
        "  skew_feat_val = df[non_obj_col].apply(lambda x : skew(x)).sort_values(ascending = False)\n",
        "  print(skew_feat_val)\n",
        "  skew_feat_val = skew_feat_val[abs(skew_feat_val) > 0.75]\n",
        "  lam = 0.3\n",
        "  for feature in skew_feat_val.index:\n",
        "    df[feature] = boxcox1p(df[feature], lam)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br2hLXsVpiJ0",
        "colab_type": "text"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60Kz24iIeuxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (20, 20))\n",
        "sns.heatmap(df_train.corr(), annot = True, square = True)\n",
        "plt.figure(figsize = (20, 20))\n",
        "sns.heatmap(df_test.corr(), annot = True, square = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrilr4UNpvwN",
        "colab_type": "text"
      },
      "source": [
        "Removing the features with correlation more than 0.85"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVFlG61LgjLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfs = [df_train, df_test]\n",
        "for df in dfs:\n",
        "  correlated_features = set()\n",
        "  corr_matrix = df.corr()\n",
        "  for i in range(len(corr_matrix.columns)):\n",
        "      for j in range(i):\n",
        "          if abs(corr_matrix.iloc[i, j]) > 0.85:\n",
        "            col_name = corr_matrix.columns[i]\n",
        "            correlated_features.add(col_name)\n",
        "  print(correlated_features)\n",
        "  df.drop(list(correlated_features), axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiYxf5f5p_Ik",
        "colab_type": "text"
      },
      "source": [
        "Getting the mutual information of the columns with dependent variable "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F9Q1tGoqiIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mutual_info = mutual_info_classif(df_train.iloc[:, 1: -1],df_train.iloc[:,-1])\n",
        "plt.subplots(1, figsize=(26, 1))\n",
        "sns.heatmap(mutual_info[:, np.newaxis].T, cmap='Blues', cbar=False, linewidths=1, annot=True)\n",
        "plt.yticks([], [])\n",
        "plt.gca().set_xticklabels(df_train.columns[1:-1], rotation=45, ha='right', fontsize=12)\n",
        "plt.suptitle('Variable Importance (mutual_info_classif)', fontsize=18, y=1.2)\n",
        "plt.gcf().subplots_adjust(wspace=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARlGKberrApG",
        "colab_type": "text"
      },
      "source": [
        "Keeping only the top 80 percentile of the columns using mutual information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UloPCwyxBMls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trans = GenericUnivariateSelect(score_func=mutual_info_classif, mode='percentile', param=80)\n",
        "train_trans = trans.fit_transform(df_train.iloc[:, 1: -1], df_train.iloc[:,-1])\n",
        "columns_retained_Select = df_train.iloc[:, 1:-1].columns[trans.get_support()].values\n",
        "print(columns_retained_Select)\n",
        "df_train_trans = df_train[columns_retained_Select]\n",
        "df_test_noID = df_test[df_train_trans.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4cAtiaUquZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_trans.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DQ4BgFuEpos",
        "colab_type": "text"
      },
      "source": [
        "One hot encoding the categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjqbSa5ef90i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
        "encoder.fit(df_train_trans[df_train_trans.select_dtypes(include = 'object').columns])\n",
        "X_trans = encoder.transform(df_train_trans[df_train_trans.select_dtypes(include = 'object').columns])\n",
        "test_trans = encoder.transform(df_test_noID[df_test_noID.select_dtypes(include = 'object').columns])\n",
        "X = pd.DataFrame(X_trans)\n",
        "test = pd.DataFrame(test_trans)\n",
        "X = pd.concat([X, df_train_trans], axis = 1)\n",
        "test = pd.concat([test, df_test_noID], axis = 1)\n",
        "X.drop(df_train_trans.select_dtypes(include = 'object').columns, axis = 1, inplace = True)\n",
        "test.drop(df_test_noID.select_dtypes(include = 'object').columns, axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N58XIGbQpL2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNMvkikzE2KK",
        "colab_type": "text"
      },
      "source": [
        "Since there are outliers in the data it is scaled with Robust Scaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUX9RUAO6nYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = RobustScaler().fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "test_scaled = scaler.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OahRg4WS7E73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_scaled, cv_X_scaled, train_y, cv_y = train_test_split(X_scaled, df_train['Dependent_Variable'], test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXHJUDu-I4YT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X, cv_X, train_y, cv_y = train_test_split(X, df_train['Dependent_Variable'], test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svmJSTdhpp4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, ShuffleSplit, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from scipy.stats import randint, uniform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLqrfqvrFP-s",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression gives out a very bad score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3DdmpMc3ukN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = [{'C' : [100]}]\n",
        "cv_split = ShuffleSplit(n_splits = 5, test_size = 0.2, train_size = 0.75, random_state = 0)\n",
        "best_search = GridSearchCV(estimator = LogisticRegression(max_iter = 10000), param_grid = param_grid, cv = cv_split, scoring = 'roc_auc')\n",
        "best_search.fit(train_X_scaled, train_y)\n",
        "print(best_search.best_score_)\n",
        "print(best_search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXEcU9l9FZR1",
        "colab_type": "text"
      },
      "source": [
        "Since there are too many rows SVC takes impractically long and cant be used. No point in using SVClinear since linear function wont fit this data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q36-9elC_YYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''param_grid = [{'C' : [0.1, 1, 10]}]\n",
        "cv_split = ShuffleSplit(n_splits = 5, test_size = 0.2, train_size = 0.75, random_state = 0)\n",
        "best_search = GridSearchCV(estimator = SVC(), param_grid = param_grid, cv = cv_split, scoring = 'roc_auc')\n",
        "best_search.fit(train_X_scaled, train_y)\n",
        "print(best_search.best_score_)\n",
        "print(best_search.best_params_)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPXfsrJ6F_D7",
        "colab_type": "text"
      },
      "source": [
        "Defining scoring function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4dCGEOhWX_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scoring(score):\n",
        "    fpr,tpr, thresholds = roc_curve(cv_y, score[:,1])\n",
        "    return(auc(fpr, tpr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we6RkZsyGMCX",
        "colab_type": "text"
      },
      "source": [
        "Using XGB with randomized search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhcEfv1Z8oT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_grid_xgb = {\n",
        "        'silent': [False],\n",
        "        'max_depth': [6, 10, 15, 20],\n",
        "        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n",
        "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
        "        'gamma': [0, 0.25, 0.5, 1.0],\n",
        "        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
        "        'n_estimators': [100, 300]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ03_kLRuioT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_split = ShuffleSplit(n_splits = 5, test_size = 0.2, train_size = 0.75, random_state = 0)\n",
        "xgb_random = RandomizedSearchCV(estimator = XGBClassifier(random_state = 38), param_distributions = random_grid_xgb, \n",
        "                               scoring = 'roc_auc', n_iter = 60, cv = cv_split, verbose=3, random_state=38, n_jobs = -1)\n",
        "xgb_random.fit(train_X, train_y)\n",
        "print(xgb_random.best_score_)\n",
        "print(xgb_random.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE81rdAKvtOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_xgb_random = xgb_random.predict_proba(cv_X)\n",
        "print(scoring(score_xgb_random))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOpp-4GEGbju",
        "colab_type": "text"
      },
      "source": [
        "Randomforestclassifier with randomized search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNGTShUgGYI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "max_features = ['auto', 'sqrt']\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf5AKDf_GtFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_split = ShuffleSplit(n_splits = 5, test_size = 0.2, train_size = 0.75, random_state = 0)\n",
        "rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(random_state = 42), param_distributions = random_grid, \n",
        "                               n_iter = 100, cv = cv_split, verbose=2, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(train_X, train_y)\n",
        "print(rf_random.best_score_)\n",
        "print(rf_random.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjNYYYaNG8Ds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_rf_random = rf_random.predict_proba(cv_X)\n",
        "print(scoring(score_rf_random))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFGPHWM8HEVX",
        "colab_type": "text"
      },
      "source": [
        "LGBM classifier with randomized search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cf_w0WyG_Gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_grid_lgbm ={'num_leaves': randint(6, 50), \n",
        "                   'learning_rate' : [0.001, 0.01, 0.1],\n",
        "             'min_child_samples': randint(20, 500), \n",
        "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
        "             'subsample': uniform(0.4, 0.8), \n",
        "             'colsample_bytree': uniform(0.4, 0.6),\n",
        "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
        "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PSGPcQNHVor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_split = ShuffleSplit(n_splits = 5, test_size = 0.2, train_size = 0.75, random_state = 0)\n",
        "lgbm_random = RandomizedSearchCV(estimator = LGBMClassifier(max_depth=-1, random_state=50, silent=False, n_estimators=5000, bagging_fraction = 1), param_distributions = random_grid_lgbm, \n",
        "                               scoring = 'roc_auc', n_iter = 60, cv = cv_split, verbose=2, random_state=50, n_jobs = -1)\n",
        "lgbm_random.fit(train_X, train_y)\n",
        "print(lgbm_random.best_score_)\n",
        "print(lgbm_random.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lLz2_UmHbCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_lgbm = lgbm_random.predict_proba(cv_X)\n",
        "print(scoring(score_lgbm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsHFUaGPHhfm",
        "colab_type": "text"
      },
      "source": [
        "Neural Network with keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxP6QIeBweOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import AUC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na-0nMYFpFXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBV6cUiNJxtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neural_net():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(180, input_dim=train_X.shape[1], activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[AUC()])\n",
        "  return model\n",
        "estimator = KerasClassifier(build_fn=neural_net, epochs=10000, batch_size=30000, verbose=1)\n",
        "history = estimator.fit(train_X, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqjC31YWYbOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_neural_net = estimator.predict_proba(cv_X)\n",
        "print(scoring(score_neural_net))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8FpRCVHo59e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktUoZxT2HyiU",
        "colab_type": "text"
      },
      "source": [
        "The best CV sroc_auc score was from LGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7kgYVCpXtqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_score = lgbm_random.predict_proba(test)\n",
        "df = pd.DataFrame()\n",
        "df['Unique_ID'] = df_test['Unique_ID']\n",
        "df['Class_1_Probability'] = out_score[:,1]\n",
        "df.to_csv('submission_file.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beiQVmEoUayG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}